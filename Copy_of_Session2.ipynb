{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Session2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rramjee/rramjee/blob/master/Copy_of_Session2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJ7YEw_vyjG",
        "colab_type": "text"
      },
      "source": [
        "# **Not an ideal network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGWv5hBhv2jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "# Installing Keras Library. This is designed to run on top of the tensorflow framework\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMlDJQKv4VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing all the required classes from numpy and keras libraries. We will be calling the methods from these classes to perform required action in our following code.\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Convolution2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CdSu2lMwB9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading MNIST Dataset and also spliting the data into Test and Train set\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLaDf0-rwCmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Printing the shape of Training set dimensions this will help to be able to pass the input layer dimesions to the neural network.\n",
        "print (X_train.shape)\n",
        "# Viewing the first image from training set to get an idea of the images in training set. \n",
        "# So it appears that we have 60,000 samples in our training set, and the images are 28 pixels x 28 pixels each. We can confirm this by plotting the first sample in matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erb11jNwwFwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All gray scale images will have only one channel and that needs to be explicity mentioned. So, we are reshaping all the training and test images to 28*28*1. \n",
        "# Specifically mentioning only one channel.\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLK4YDoRwHet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since it is a gray scale image. All the values will be in the range of 0 to 255. Hence normalizing the values to the range of 0 to 1.\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKLOmhlwJQl",
        "colab_type": "code",
        "outputId": "6d702fa4-2c12-4081-f20c-8f5584b88b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Checking the values of the first ten images in the training set\n",
        "y_train[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YusMJguiwKsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using categorical function to get a one hot encoding of the output values. This will help perform the loss function calculations \n",
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upxc99AswMW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Looking at the first ten rows of one hot encoded values of training set\n",
        "Y_train[:10]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTVUE47wNwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Building the Neural Network layers sequentially and defining the input shape, number of kernals, max pooling layers, etc.\n",
        "# Relu is used as activation function in the internal layers and softmax is used for classification in the last layer\n",
        "from keras.layers import Activation, MaxPooling2D\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1)))  # i/p : 28*28*1 rec : 3*3 o/p : 26*26*32\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu')) # i/p : 26*26*32 rec : 5*5 o/p : 24*24*64\n",
        "model.add(Convolution2D(128, 3, 3, activation='relu')) # i/p : 24*24*64 rec : 7*7 o/p : 22*22*128\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # i/p : 22*22*128 rec : 14*14 o/p : 11*11*128\n",
        "\n",
        "model.add(Convolution2D(256, 3, 3, activation='relu')) # i/p : 11*11*128 rec : 16*16 o/p : 9*9*256\n",
        "model.add(Convolution2D(512, 3, 3, activation='relu')) # i/p : 9*9*256 rec : 18*18 o/p : 7*7*512\n",
        "model.add(Convolution2D(1024, 3, 3, activation='relu')) # i/p : 7*7*512 rec : 20*20 o/p : 5*5*1024\n",
        "model.add(Convolution2D(2048, 3, 3, activation='relu')) # i/p : 5*5*1024 rec : 22*22 o/p : 3*3*2048\n",
        "model.add(Convolution2D(10, 3, 3, activation='relu')) # i/p : 3*3*2048 rec : 24*24 o/p : 1*1*10\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOpRb6yG7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model by sepcifying the optimizer function, loss function and metrics.\n",
        "# adam is generally the preferred optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O248wVQyMft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a68cf8e5-7d18-4529-8a64-ad05fd013917"
      },
      "source": [
        "# Fit functiona is called to train the network with the training dataset with 10 epcohs\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3025 - acc: 0.0988\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 113s 2ms/step - loss: 2.3026 - acc: 0.0987\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 114s 2ms/step - loss: 2.3026 - acc: 0.0987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f38a05565f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sst4KneiyOL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluating the trained model by testing the against the test data to get accuracy and other metrics\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJiXOKsyj4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d5e16df-0751-41ab-f1cc-04addc0fb759"
      },
      "source": [
        "print(score)\n",
        "#printing score"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.3025851249694824, 0.098]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwLSXt7nyn_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "#predicting the data on test set and storing in y pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWKKoOKwyppN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a4b4f6b8-ba23-40a9-f0bb-c04978cc26fd"
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
            " [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPhMjpUUEpbw",
        "colab_type": "text"
      },
      "source": [
        "##What according to me is wrong with the network\n",
        "\n",
        "\n",
        "1.   I find that the number of channels(kernels) used at the later layers are way too much for problem of classifiying the digits 0 to 9 in gray scale. In my view, this could be achieved by using lesser number of channels (kernels). This also oincreases the number of parameters significantly.\n",
        "2.   The penultimate layer has 2048 channels and immediately there is a drop to 10 channels in the last layer. This could lead to missing lot of important features and information. Instead it should have been gradually decreased.\n",
        "3. a 3*3 kernel is used in the last layer that has an input size of 3*3*2048. It is usally not a good idea to use a 3*3 kernel on a 3*3 input to get to a 1*1 output. you will loose a lot of critical information. Instead the maxpooling could have been done is to use a max pooling at 20*20 to get to a 10*10 layer and reduced it to a 2*2 and use a 2*2 filter to reach 1*1. This way curcuial information will not be lost.\n",
        "\n"
      ]
    }
  ]
}